# -*- coding: utf-8 -*-
"""GRAD699 Code Freeze.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tH_cG42K9SHehkgXPc0dAGx1i7dwDGrR
"""

!pip install optuna

import pandas as pd
import numpy as np
#from factor_analyzer import FactorAnalyzer
#from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,
                            roc_curve, mean_squared_error, mean_absolute_error, r2_score,
                            accuracy_score, f1_score)
import xgboost as xgb
from xgboost import plot_importance
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

df = pd.read_csv('/content/drive/MyDrive/GRAD 695/NFCS 2021 State Data 220627.csv')

# Filter for young adults (18-34)
young_adults = df[df['A3Ar_w'].isin([1, 2])].copy()  # 1=18-24, 2=25-34
analysis_df = young_adults.copy()

exclude_vars = [
    'NFCSID',        # Respondent ID - unique identifier
    'STATEQ',        # State - geographic identifier
    'CENSUSDIV',     # Census Division - geographic grouping
    'CENSUSREG',     # Census Region - broader geographic grouping
    'X3',            # Questionnaire Version Variable - survey administration detail
    'wgt_n2',        # National-level weight
    'wgt_d2',        # Census Division-level weight
    'wgt_s3'         # State-level weight
]
analysis_df = analysis_df.drop(columns=[col for col in exclude_vars if col in analysis_df.columns], errors='ignore')

response_codes = [98, 99]  # "Don't know" and "Prefer not to say"
rare_threshold=0.05
# Replace response codes with NaN in numeric columns
for col in analysis_df.columns:
    if analysis_df[col].dtype in ['int64', 'float64']:
        analysis_df[col] = analysis_df[col].replace(response_codes, np.nan)

# Handle categorical variables
print("Processing categorical variables...")
categorical_cols = []
numerical_cols = []

for col in analysis_df.columns:
    unique_vals = analysis_df[col].nunique()
    if unique_vals <= 20 and analysis_df[col].dtype == 'object':
        categorical_cols.append(col)
    elif unique_vals <= 20:
        # Likely ordinal/categorical even if numeric
        categorical_cols.append(col)
    else:
        numerical_cols.append(col)

# Process categorical variables
le_dict = {}
for col in categorical_cols:
    if col in analysis_df.columns:
        # Handle rare categories
        value_counts = analysis_df[col].value_counts(normalize=True)
        rare_values = value_counts[value_counts < rare_threshold].index.tolist()

        if len(rare_values) > 0 and len(value_counts) > 2:
            analysis_df[col] = analysis_df[col].replace(rare_values, 'Other')

        # Label encode
        le = LabelEncoder()
        mask = analysis_df[col].notna()
        if mask.sum() > 0:
            analysis_df.loc[mask, col] = le.fit_transform(
                analysis_df.loc[mask, col].astype(str)
            )
            le_dict[col] = le

# Impute missing values
print("Imputing missing values...")
# Separate numerical and categorical for different imputation strategies
num_imputer = SimpleImputer(strategy='median')
cat_imputer = SimpleImputer(strategy='most_frequent')

if numerical_cols:
    num_data = analysis_df[numerical_cols]
    analysis_df[numerical_cols] = num_imputer.fit_transform(num_data)

if categorical_cols:
    cat_data = analysis_df[[col for col in categorical_cols if col in analysis_df.columns]]
    analysis_df[cat_data.columns] = cat_imputer.fit_transform(cat_data)

# Convert all to numeric
for col in analysis_df.columns:
    analysis_df[col] = pd.to_numeric(analysis_df[col], errors='coerce')

# Final cleanup
analysis_df = analysis_df.dropna(axis=1, how='all')  # Remove columns that are all NaN
analysis_df = analysis_df.fillna(analysis_df.median())  # Fill any remaining NaN with median

import hashlib

# ============================================================================
# SEED GENERATION
# ============================================================================

def md5_hash(input_string):
    """Generate MD5 hash from string"""
    md5_hasher = hashlib.md5()
    md5_hasher.update(input_string.encode('utf-8'))
    return md5_hasher.hexdigest()

def generate_seed(seed_string='credit_default_pipeline'):
    """
    Generate deterministic seed from hash

    Parameters:
    - seed_string: Base string for hashing

    Returns:
    - Single seed value
    """
    hashed_value = md5_hash(seed_string)
    base_number = int(hashed_value, 16)

    # Keep within valid range for random seed
    seed = base_number % (2**31 - 1)

    print(f"Using seed string: '{seed_string}'")
    print(f"Generated seed: {seed}")

    return seed
# ============================================================================
# STEP 1: DATA PREPARATION - EXCLUDE TRANSACTIONAL FEATURES
# ============================================================================

def prepare_nontransactional_data(df):
    """
    Remove credit card transactional features and create binary target

    Parameters:
    - df: Original NFCS dataframe

    Returns:
    - Cleaned dataframe with non-transactional features only
    """
    print("="*70)
    print("STEP 1: DATA PREPARATION")
    print("="*70)

    df_clean = df.copy()

    # Define credit card transactional variables to EXCLUDE
    credit_card_transactional = [
        'F2_1',  # Paid in full - transactional behavior
        'F2_2',  # Carried balance - transactional behavior
        'F2_3',  # Minimum payment - transactional behavior
        'F2_5',  # Over-limit fees - transactional outcome
        'F2_6',  # Cash advances - transactional behavior
    ]

    # Additional credit-related outcomes to exclude
    credit_outcomes = [
        'G35',   # Late with student loan payment
        'G38',   # Debt collection contact
        'C10_2012',  # Loan from retirement (financial distress outcome)
        'C11_2012',  # Hardship withdrawal (financial distress outcome)
    ]

    # Variables to KEEP (non-transactional predictors)
    # These are available BEFORE someone has credit card problems

    print("Excluding transactional/outcome variables:")
    excluded_vars = credit_card_transactional + credit_outcomes
    for var in excluded_vars:
        if var in df_clean.columns:
            print(f"  - {var}")

    # Remove excluded variables
    df_clean = df_clean.drop(columns=[v for v in excluded_vars if v in df_clean.columns])

    print(f"\nOriginal shape: {df.shape}")
    print(f"After exclusions: {df_clean.shape}")

    return df_clean

def create_binary_target(df, original_target='F2_4'):
    """
    Create binary target: Had late fees (1) vs. No late fees (0)

    Parameters:
    - df: Dataframe
    - original_target: Original F2_4 variable name

    Returns:
    - Dataframe with binary target added
    """
    print("\n" + "="*70)
    print("STEP 2: CREATE BINARY TARGET")
    print("="*70)

    df_binary = df.copy()

    # Original F2_4 values:
    # 0 = Not selected/Never
    # 1 = Yes/Had late fees (THIS IS OUR TARGET)
    # 2 = No
    # 3 = Don't know

    # Create binary: 1 = Yes (had late fees), 0 = All others
    df_binary['F2_4_binary'] = (df_binary[original_target] == 1).astype(int)

    print(f"Original F2_4 distribution:")
    print(df[original_target].value_counts().sort_index())

    print(f"\nBinary target distribution:")
    print(df_binary['F2_4_binary'].value_counts())

    prevalence = df_binary['F2_4_binary'].mean() * 100
    print(f"\nLate fee prevalence: {prevalence:.1f}%")
    print(f"Class balance: {100-prevalence:.1f}% no late fees, {prevalence:.1f}% late fees")

    if prevalence < 20:
        print("\n⚠ Warning: Imbalanced classes detected")
        print("  Consider using class_weight='balanced' in models")

    return df_binary

# ============================================================================
# STEP 3: XGBOOST FEATURE SELECTION
# ============================================================================

def xgboost_feature_selection(df, target_col='F2_4_binary', top_n=30):
    """
    Run XGBoost feature selection on non-transactional features

    Parameters:
    - df: Dataframe with features and binary target
    - target_col: Target variable name
    - top_n: Number of top features to return

    Returns:
    - Dictionary with model and feature importance
    """
    print("\n" + "="*70)
    print("STEP 3: XGBOOST FEATURE SELECTION")
    print("="*70)

    # Prepare data
    y = df[target_col].dropna()
    X = df.drop(columns=[target_col, 'F2_4']).select_dtypes(include=[np.number])
    X = X.loc[y.index]

    # Handle missing values
    X = X.fillna(X.median())

    print(f"Features shape: {X.shape}")
    print(f"Target distribution: {y.value_counts().to_dict()}")

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"\nTraining samples: {len(X_train)}")
    print(f"Test samples: {len(X_test)}")

    # Train XGBoost with class weights for imbalance
    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
    print(f"Scale pos weight: {scale_pos_weight:.2f}")

    model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        scale_pos_weight=scale_pos_weight,  # Handle imbalance
        random_state=42,
        eval_metric='logloss'
    )

    print("\nTraining XGBoost model...")
    model.fit(X_train, y_train,
              eval_set=[(X_test, y_test)],
              verbose=False)

    # Evaluate
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]

    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba)

    print(f"\n--- Model Performance ---")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1-Score: {f1:.4f}")
    print(f"ROC-AUC: {roc_auc:.4f}")

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred,
                                target_names=['No Late Fees', 'Late Fees']))

    # Feature importance
    importance_dict = model.get_booster().get_score(importance_type='gain')

    # Map feature names
    feature_names = X.columns.tolist()
    importance_df = pd.DataFrame({
        'Feature': [feature_names[int(k[1:])] if k.startswith('f') else k
                   for k in importance_dict.keys()],
        'Importance': list(importance_dict.values())
    }).sort_values('Importance', ascending=False).reset_index(drop=True)

    # Calculate percentages
    importance_df['Importance_Pct'] = (importance_df['Importance'] /
                                       importance_df['Importance'].sum()) * 100
    importance_df['Cumulative_Pct'] = importance_df['Importance_Pct'].cumsum()

    print(f"\n--- Top {min(top_n, len(importance_df))} Features by Importance ---")
    print(importance_df.head(top_n).to_string(index=False))

    # Visualize
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))

    # Top features bar plot
    ax1 = axes[0]
    top_feats = importance_df.head(30)
    ax1.barh(range(len(top_feats)), top_feats['Importance'])
    ax1.set_yticks(range(len(top_feats)))
    ax1.set_yticklabels(top_feats['Feature'])
    ax1.invert_yaxis()
    ax1.set_xlabel('Importance Score')
    ax1.set_title('Top 30 Features by Importance')
    ax1.grid(True, alpha=0.3)

    # Cumulative importance
    ax2 = axes[1]
    ax2.plot(range(1, len(importance_df) + 1), importance_df['Cumulative_Pct'],
             'b-', linewidth=2)
    ax2.axhline(y=80, color='r', linestyle='--', label='80% threshold')
    ax2.axhline(y=90, color='orange', linestyle='--', label='90% threshold')
    ax2.set_xlabel('Number of Features')
    ax2.set_ylabel('Cumulative Importance (%)')
    ax2.set_title('Cumulative Feature Importance')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    return {
        'model': model,
        'importance_df': importance_df,
        'X_train': X_train,
        'X_test': X_test,
        'y_train': y_train,
        'y_test': y_test,
        'accuracy': accuracy,
        'f1_score': f1,
        'roc_auc': roc_auc
    }

# ============================================================================
# STEP 4: DETERMINE OPTIMAL NUMBER OF FEATURES
# ============================================================================

def find_optimal_features(df, target_col, importance_df,
                         feature_counts=[5, 10, 15, 20, 25, 30,5,40,45,50]):
    """
    Test different numbers of features to find optimal subset
    """
    print("\n" + "="*70)
    print("STEP 4: DETERMINE OPTIMAL NUMBER OF FEATURES")
    print("="*70)

    y = df[target_col].dropna()
    all_features = importance_df['Feature'].tolist()

    results = []

    for n in feature_counts:
        if n > len(all_features):
            continue

        print(f"\nTesting {n} features...")

        selected_features = all_features[:n]
        X = df[selected_features].fillna(df[selected_features].median())
        X = X.loc[y.index]

        # Train-test split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        # Train model
        scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
        model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            scale_pos_weight=scale_pos_weight,
            random_state=42
        )
        model.fit(X_train, y_train, verbose=False)

        # Evaluate
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]

        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        roc_auc = roc_auc_score(y_test, y_pred_proba)

        # Cross-validation
        cv_scores = cross_val_score(model, X, y, cv=5, scoring='roc_auc')

        print(f"  Accuracy: {accuracy:.4f}")
        print(f"  F1-Score: {f1:.4f}")
        print(f"  ROC-AUC: {roc_auc:.4f}")
        print(f"  CV ROC-AUC: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}")

        results.append({
            'n_features': n,
            'accuracy': accuracy,
            'f1_score': f1,
            'roc_auc': roc_auc,
            'cv_roc_auc_mean': cv_scores.mean(),
            'cv_roc_auc_std': cv_scores.std()
        })

    results_df = pd.DataFrame(results)

    print("\n--- Summary ---")
    print(results_df.to_string(index=False))

    # Recommendation
    best_f1_idx = results_df['f1_score'].idxmax()
    best_auc_idx = results_df['roc_auc'].idxmax()

    print("\n--- Recommendation ---")
    print(f"Best F1-Score: {results_df.loc[best_f1_idx, 'n_features']:.0f} features")
    print(f"Best ROC-AUC: {results_df.loc[best_auc_idx, 'n_features']:.0f} features")

    results_df = results_df.sort_values('n_features')

    # Visualize
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    ax1 = axes[0]
    ax1.plot(results_df['n_features'], results_df['accuracy'], 'bo-', linewidth=2)
    ax1.set_xlabel('Number of Features')
    ax1.set_ylabel('Accuracy')
    ax1.set_title('Accuracy vs Features')
    ax1.grid(True, alpha=0.3)

    ax2 = axes[1]
    ax2.plot(results_df['n_features'], results_df['f1_score'], 'go-', linewidth=2)
    ax2.set_xlabel('Number of Features')
    ax2.set_ylabel('F1-Score')
    ax2.set_title('F1-Score vs Features')
    ax2.grid(True, alpha=0.3)

    ax3 = axes[2]
    ax3.plot(results_df['n_features'], results_df['roc_auc'], 'ro-', linewidth=2)
    ax3.errorbar(results_df['n_features'], results_df['cv_roc_auc_mean'],
                yerr=results_df['cv_roc_auc_std'], fmt='o', alpha=0.5)
    ax3.set_xlabel('Number of Features')
    ax3.set_ylabel('ROC-AUC')
    ax3.set_title('ROC-AUC vs Features')
    ax3.legend(['Test', 'CV'])
    ax3.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    return results_df

# ============================================================================
# STEP 5: CLASSIFICATION VS REGRESSION COMPARISON
# ============================================================================

def compare_xgboost_approaches(df, target_col, selected_features):
    """
    Compare XGBoost Classifier vs XGBoost Regressor
    """
    print("\n" + "="*70)
    print("STEP 5: XGBOOST CLASSIFICATION VS REGRESSION")
    print("="*70)

    # Prepare data
    y = df[target_col].dropna()
    X = df[selected_features].fillna(df[selected_features].median())
    X = X.loc[y.index]

    print(f"Using {len(selected_features)} features")
    print(f"Dataset shape: {X.shape}")

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    results = {}

    # ==================== CLASSIFICATION ====================
    print("\n" + "-"*70)
    print("XGBOOST CLASSIFIER")
    print("-"*70)

    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

    clf = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        scale_pos_weight=scale_pos_weight,
        random_state=42,
        eval_metric='logloss'
    )

    clf.fit(X_train, y_train, verbose=False)

    y_pred_clf = clf.predict(X_test)
    y_pred_proba_clf = clf.predict_proba(X_test)[:, 1]

    accuracy_clf = accuracy_score(y_test, y_pred_clf)
    f1_clf = f1_score(y_test, y_pred_clf)
    roc_auc_clf = roc_auc_score(y_test, y_pred_proba_clf)

    print(f"Accuracy: {accuracy_clf:.4f}")
    print(f"F1-Score: {f1_clf:.4f}")
    print(f"ROC-AUC: {roc_auc_clf:.4f}")

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred_clf,
                                target_names=['No Late Fees', 'Late Fees']))

    results['classifier'] = {
        'model': clf,
        'accuracy': accuracy_clf,
        'f1_score': f1_clf,
        'roc_auc': roc_auc_clf,
        'y_pred': y_pred_clf,
        'y_pred_proba': y_pred_proba_clf
    }

    # ==================== REGRESSION ====================
    print("\n" + "-"*70)
    print("XGBOOST REGRESSOR (with rounded predictions)")
    print("-"*70)

    reg = xgb.XGBRegressor(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        random_state=42
    )

    reg.fit(X_train, y_train, verbose=False)

    y_pred_reg = reg.predict(X_test)
    y_pred_reg_rounded = np.round(np.clip(y_pred_reg, 0, 1)).astype(int)

    # Regression metrics
    mse = mean_squared_error(y_test, y_pred_reg)
    mae = mean_absolute_error(y_test, y_pred_reg)
    r2 = r2_score(y_test, y_pred_reg)

    # Classification metrics on rounded predictions
    accuracy_reg = accuracy_score(y_test, y_pred_reg_rounded)
    f1_reg = f1_score(y_test, y_pred_reg_rounded)

    # For ROC-AUC, use raw predictions as probabilities
    y_pred_proba_reg = np.clip(y_pred_reg, 0, 1)
    roc_auc_reg = roc_auc_score(y_test, y_pred_proba_reg)

    print(f"MSE: {mse:.4f}")
    print(f"MAE: {mae:.4f}")
    print(f"R² Score: {r2:.4f}")
    print(f"\nAccuracy (rounded): {accuracy_reg:.4f}")
    print(f"F1-Score (rounded): {f1_reg:.4f}")
    print(f"ROC-AUC: {roc_auc_reg:.4f}")

    print("\nClassification Report (rounded predictions):")
    print(classification_report(y_test, y_pred_reg_rounded,
                                target_names=['No Late Fees', 'Late Fees']))

    results['regressor'] = {
        'model': reg,
        'mse': mse,
        'mae': mae,
        'r2': r2,
        'accuracy': accuracy_reg,
        'f1_score': f1_reg,
        'roc_auc': roc_auc_reg,
        'y_pred': y_pred_reg_rounded,
        'y_pred_proba': y_pred_proba_reg
    }

    # ==================== COMPARISON ====================
    print("\n" + "="*70)
    print("COMPARISON SUMMARY")
    print("="*70)

    comparison = pd.DataFrame({
        'Metric': ['Accuracy', 'F1-Score', 'ROC-AUC'],
        'Classifier': [accuracy_clf, f1_clf, roc_auc_clf],
        'Regressor': [accuracy_reg, f1_reg, roc_auc_reg],
        'Difference': [accuracy_clf - accuracy_reg,
                      f1_clf - f1_reg,
                      roc_auc_clf - roc_auc_reg]
    })

    print(comparison.to_string(index=False))

    # Recommendation
    print("\n--- Recommendation ---")
    if abs(comparison['Difference'].mean()) < 0.01:
        print("Performance is essentially identical (<1% difference)")
        print("✓ Use CLASSIFIER for:")
        print("  - Interpretable class probabilities")
        print("  - Built-in class weight handling")
        print("  - Standard binary classification workflow")
    elif comparison['Difference'].mean() > 0:
        print("✓ Use CLASSIFIER (better performance)")
    else:
        print("✓ Use REGRESSOR (better performance)")

    # Visualize comparison
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    # ROC curves
    ax1 = axes[0]
    fpr_clf, tpr_clf, _ = roc_curve(y_test, y_pred_proba_clf)
    fpr_reg, tpr_reg, _ = roc_curve(y_test, y_pred_proba_reg)

    ax1.plot(fpr_clf, tpr_clf, 'b-', linewidth=2,
            label=f'Classifier (AUC={roc_auc_clf:.3f})')
    ax1.plot(fpr_reg, tpr_reg, 'r-', linewidth=2,
            label=f'Regressor (AUC={roc_auc_reg:.3f})')
    ax1.plot([0, 1], [0, 1], 'k--', alpha=0.3)
    ax1.set_xlabel('False Positive Rate')
    ax1.set_ylabel('True Positive Rate')
    ax1.set_title('ROC Curves')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Confusion matrices
    ax2 = axes[1]
    cm_clf = confusion_matrix(y_test, y_pred_clf)
    sns.heatmap(cm_clf, annot=True, fmt='d', cmap='Blues', ax=ax2)
    ax2.set_title('Classifier Confusion Matrix')
    ax2.set_xlabel('Predicted')
    ax2.set_ylabel('Actual')

    ax3 = axes[2]
    cm_reg = confusion_matrix(y_test, y_pred_reg_rounded)
    sns.heatmap(cm_reg, annot=True, fmt='d', cmap='Oranges', ax=ax3)
    ax3.set_title('Regressor Confusion Matrix')
    ax3.set_xlabel('Predicted')
    ax3.set_ylabel('Actual')

    plt.tight_layout()
    plt.show()

    return results, comparison

# ============================================================================
# COMPLETE WORKFLOW
# ============================================================================

def run_complete_pipeline(df, top_n_features=20):
    """
    Run complete credit default prediction pipeline

    Parameters:
    - df: Original NFCS dataframe
    - top_n_features: Number of top features to use

    Returns:
    - Dictionary with all results
    """
    print("="*70)
    print("COMPLETE CREDIT CARD DEFAULT PREDICTION PIPELINE")
    print("Using Non-Transactional Data Only")
    print("="*70)

    # Step 1: Remove transactional features
    df_clean = prepare_nontransactional_data(df)

    # Step 2: Create binary target
    df_binary = create_binary_target(df_clean)

    # Step 3: XGBoost feature selection
    selection_results = xgboost_feature_selection(
        df_binary,
        target_col='F2_4_binary',
        top_n=50
    )

    # Step 4: Find optimal number of features
    optimal_results = find_optimal_features(
        df_binary,
        'F2_4_binary',
        selection_results['importance_df'],
        feature_counts=[5, 10, 15, 20, 25, 30,5,40,45,50]
    )

    # Step 5: Compare classification vs regression with optimal features
    selected_features = selection_results['importance_df'].head(top_n_features)['Feature'].tolist()

    comparison_results, comparison_df = compare_xgboost_approaches(
        df_binary,
        'F2_4_binary',
        selected_features
    )

    print("\n" + "="*70)
    print("PIPELINE COMPLETE!")
    print("="*70)
    print(f"\nFinal selected features ({len(selected_features)}):")
    print(selected_features)

    # Export results
    selection_results['importance_df'].to_csv('feature_importance.csv', index=False)
    optimal_results.to_csv('optimal_features_comparison.csv', index=False)
    comparison_df.to_csv('classifier_vs_regressor.csv', index=False)
    print("\n✓ Results exported to CSV files")

    return {
        'cleaned_data': df_binary,
        'selected_features': selected_features,
        'feature_importance': selection_results['importance_df'],
        'optimal_results': optimal_results,
        'comparison_results': comparison_results,
        'comparison_df': comparison_df
    }

results = run_complete_pipeline(analysis_df, top_n_features=30)

"""
Multi-Seed Stability Analysis and Optuna Hyperparameter Optimization
For Credit Card Default Prediction
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report
import xgboost as xgb
import optuna
from optuna.visualization import plot_optimization_history, plot_param_importances
import hashlib
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# SEED GENERATION
# ============================================================================

def md5_hash(input_string):
    """Generate MD5 hash from string"""
    md5_hasher = hashlib.md5()
    md5_hasher.update(input_string.encode('utf-8'))
    return md5_hasher.hexdigest()

def generate_seeds(seed_string='credit_default_stability', n_seeds=10):
    """
    Generate deterministic seeds from hash

    Parameters:
    - seed_string: Base string for hashing
    - n_seeds: Number of seeds to generate

    Returns:
    - List of seed values
    """
    print("="*70)
    print("GENERATING DETERMINISTIC SEEDS")
    print("="*70)

    hashed_value = md5_hash(seed_string)
    base_number = int(hashed_value, 16)

    print(f"Base string: '{seed_string}'")
    print(f"MD5 hash: {hashed_value}")
    print(f"Base seed: {base_number}")

    # Generate seeds by adding increments
    seeds = [(base_number + i * 1000) % (2**31 - 1) for i in range(n_seeds)]

    print(f"\nGenerated {n_seeds} seeds:")
    for i, seed in enumerate(seeds[:5]):  # Show first 5
        print(f"  Seed {i+1}: {seed}")
    if n_seeds > 5:
        print(f"  ... and {n_seeds-5} more")

    return seeds
my_seed = generate_seeds('Qihang', n_seeds=1)[0]
# ============================================================================
# MULTI-SEED VARIANCE ANALYSIS
# ============================================================================

def multiseed_variance_analysis(df, target_col, features, seeds=None, n_seeds=10):
    """
    Test model stability across multiple random seeds

    Parameters:
    - df: DataFrame with features and target
    - target_col: Target variable name
    - features: List of feature names to use
    - seeds: List of seeds (if None, generates them)
    - n_seeds: Number of seeds if generating

    Returns:
    - DataFrame with results for each seed
    """
    print("\n" + "="*70)
    print("MULTI-SEED VARIANCE ANALYSIS")
    print("="*70)

    if seeds is None:
        seeds = generate_seeds('F2_4_binary_variance', n_seeds)

    print(f"\nTesting model with {len(features)} features across {len(seeds)} seeds")
    print(f"Features: {features}\n")

    # Prepare data
    y = df[target_col].dropna()
    X = df[features].fillna(df[features].median())
    X = X.loc[y.index]

    results = []

    for i, seed in enumerate(seeds):
        print(f"Seed {i+1}/{len(seeds)} (seed={seed})...", end=' ')

        # Split with specific seed
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=seed, stratify=y
        )

        # Train model
        scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

        model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            scale_pos_weight=scale_pos_weight,
            random_state=seed,
            eval_metric='logloss'
        )

        model.fit(X_train, y_train, verbose=False)

        # Evaluate
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]

        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        roc_auc = roc_auc_score(y_test, y_pred_proba)

        # Get precision and recall for late fees class
        from sklearn.metrics import precision_score, recall_score
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)

        print(f"Acc={accuracy:.4f}, F1={f1:.4f}, AUC={roc_auc:.4f}")

        results.append({
            'seed': seed,
            'seed_idx': i + 1,
            'accuracy': accuracy,
            'f1_score': f1,
            'precision': precision,
            'recall': recall,
            'roc_auc': roc_auc
        })

    results_df = pd.DataFrame(results)

    # Summary statistics
    print("\n" + "="*70)
    print("VARIANCE ANALYSIS SUMMARY")
    print("="*70)

    for metric in ['accuracy', 'f1_score', 'precision', 'recall', 'roc_auc']:
        mean = results_df[metric].mean()
        std = results_df[metric].std()
        cv = std / mean  # Coefficient of variation
        min_val = results_df[metric].min()
        max_val = results_df[metric].max()

        print(f"\n{metric.upper().replace('_', ' ')}:")
        print(f"  Mean: {mean:.4f}")
        print(f"  Std Dev: {std:.4f}")
        print(f"  CV: {cv:.4f} {'✓ Stable' if cv < 0.05 else '⚠ Moderate' if cv < 0.10 else '✗ Unstable'}")
        print(f"  Range: [{min_val:.4f}, {max_val:.4f}]")

    # Overall stability assessment
    avg_cv = results_df[['accuracy', 'f1_score', 'roc_auc']].apply(
        lambda x: x.std() / x.mean()
    ).mean()

    print(f"\n{'='*70}")
    print("OVERALL STABILITY ASSESSMENT")
    print('='*70)
    print(f"Average CV across metrics: {avg_cv:.4f}")

    if avg_cv < 0.05:
        print("✓ HIGHLY STABLE - Results are very consistent across seeds")
    elif avg_cv < 0.10:
        print("✓ STABLE - Results are reasonably consistent")
    else:
        print("⚠ UNSTABLE - Results vary significantly across seeds")
        print("  Consider: more data, feature engineering, or ensemble methods")

    return results_df

def visualize_seed_variance(results_df):
    """Visualize variance across seeds"""

    fig, axes = plt.subplots(2, 3, figsize=(16, 10))

    metrics = ['accuracy', 'f1_score', 'precision', 'recall', 'roc_auc']
    metric_names = ['Accuracy', 'F1-Score', 'Precision', 'Recall', 'ROC-AUC']

    for idx, (metric, name) in enumerate(zip(metrics, metric_names)):
        if idx >= 5:
            break

        row = idx // 3
        col = idx % 3
        ax = axes[row, col]

        # Scatter plot with mean line
        ax.scatter(results_df['seed_idx'], results_df[metric], s=100, alpha=0.6)
        mean_val = results_df[metric].mean()
        ax.axhline(mean_val, color='red', linestyle='--', linewidth=2,
                  label=f'Mean: {mean_val:.4f}')

        # Add shaded region for ±1 std
        std_val = results_df[metric].std()
        ax.axhspan(mean_val - std_val, mean_val + std_val,
                  alpha=0.2, color='red', label=f'±1 SD: {std_val:.4f}')

        ax.set_xlabel('Seed Index')
        ax.set_ylabel(name)
        ax.set_title(f'{name} Across Seeds')
        ax.legend()
        ax.grid(True, alpha=0.3)

    # Distribution comparison
    ax = axes[1, 2]
    data_for_box = [results_df[m].values for m in metrics]
    bp = ax.boxplot(data_for_box, labels=metric_names, patch_artist=True)

    for patch in bp['boxes']:
        patch.set_facecolor('lightblue')

    ax.set_ylabel('Score')
    ax.set_title('Distribution of All Metrics')
    ax.tick_params(axis='x', rotation=45)
    ax.grid(True, alpha=0.3, axis='y')

    plt.tight_layout()
    plt.show()

# ============================================================================
# OPTUNA HYPERPARAMETER OPTIMIZATION (Single Seed)
# ============================================================================

def optimize_with_optuna(df, target_col, features, n_trials=50, seed=42):
    """
    Optimize XGBoost hyperparameters using Optuna with a single seed

    Parameters:
    - df: DataFrame
    - target_col: Target variable
    - features: List of features
    - n_trials: Number of optimization trials
    - seed: Random seed for reproducibility

    Returns:
    - Best parameters and study object
    """
    print("\n" + "="*70)
    print("OPTUNA HYPERPARAMETER OPTIMIZATION")
    print("="*70)
    print(f"Optimizing with seed={seed}")
    print(f"Running {n_trials} trials...\n")

    # Prepare data
    y = df[target_col].dropna()
    X = df[features].fillna(df[features].median())
    X = X.loc[y.index]

    # Split data with fixed seed
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=seed, stratify=y
    )

    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

    # Define objective function
    def objective(trial):
        # Suggest hyperparameters
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 50, 300),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'gamma': trial.suggest_float('gamma', 0, 5),
            'reg_alpha': trial.suggest_float('reg_alpha', 0, 5),
            'reg_lambda': trial.suggest_float('reg_lambda', 0, 5),
            'scale_pos_weight': scale_pos_weight,
            'random_state': seed,
            'eval_metric': 'logloss'
        }

        # Cross-validation
        model = xgb.XGBClassifier(**params)

        # Use ROC-AUC as optimization metric (more robust than F1)
        cv_scores = cross_val_score(
            model, X_train, y_train,
            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=seed),
            scoring='roc_auc',
            n_jobs=-1
        )

        return cv_scores.mean()

    # Run optimization
    study = optuna.create_study(
        direction='maximize',
        sampler=optuna.samplers.TPESampler(seed=seed)
    )

    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)

    # Results
    print("\n" + "="*70)
    print("OPTIMIZATION RESULTS")
    print("="*70)
    print(f"Best ROC-AUC: {study.best_value:.4f}")
    print(f"Best parameters:")
    for key, value in study.best_params.items():
        print(f"  {key}: {value}")

    # Train final model with best parameters
    best_params = study.best_params.copy()
    best_params['scale_pos_weight'] = scale_pos_weight
    best_params['random_state'] = seed
    best_params['eval_metric'] = 'logloss'

    best_model = xgb.XGBClassifier(**best_params)
    best_model.fit(X_train, y_train, verbose=False)

    # Evaluate on test set
    y_pred = best_model.predict(X_test)
    y_pred_proba = best_model.predict_proba(X_test)[:, 1]

    print("\n" + "="*70)
    print("TEST SET PERFORMANCE (Best Model)")
    print("="*70)
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
    print(f"F1-Score: {f1_score(y_test, y_pred):.4f}")
    print(f"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred,
                                target_names=['No Late Fees', 'Late Fees']))

    return {
        'study': study,
        'best_params': study.best_params,
        'best_model': best_model,
        'X_train': X_train,
        'X_test': X_test,
        'y_train': y_train,
        'y_test': y_test
    }

def visualize_optuna_results(study):
    """Visualize Optuna optimization results"""

    fig, axes = plt.subplots(1, 2, figsize=(15, 5))

    # Optimization history
    ax1 = axes[0]
    trials_df = study.trials_dataframe()
    ax1.plot(trials_df['number'], trials_df['value'], 'b-', alpha=0.3)
    ax1.plot(trials_df['number'], trials_df['value'].cummax(), 'r-', linewidth=2)
    ax1.set_xlabel('Trial')
    ax1.set_ylabel('ROC-AUC')
    ax1.set_title('Optimization History')
    ax1.legend(['Trial Value', 'Best So Far'])
    ax1.grid(True, alpha=0.3)

    # Parameter importance
    ax2 = axes[1]
    importances = optuna.importance.get_param_importances(study)
    params = list(importances.keys())
    values = list(importances.values())

    # Sort by importance
    sorted_pairs = sorted(zip(params, values), key=lambda x: x[1], reverse=True)
    params_sorted = [p[0] for p in sorted_pairs][:10]  # Top 10
    values_sorted = [p[1] for p in sorted_pairs][:10]

    ax2.barh(range(len(params_sorted)), values_sorted)
    ax2.set_yticks(range(len(params_sorted)))
    ax2.set_yticklabels(params_sorted)
    ax2.invert_yaxis()
    ax2.set_xlabel('Importance')
    ax2.set_title('Hyperparameter Importance')
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

# ============================================================================
# MULTI-SEED TESTING WITH OPTIMIZED PARAMETERS
# ============================================================================

def test_optimized_model_stability(df, target_col, features, best_params,
                                   seeds=None, n_seeds=10):
    """
    Test the optimized model across multiple seeds to verify stability

    Parameters:
    - df: DataFrame
    - target_col: Target variable
    - features: List of features
    - best_params: Best parameters from Optuna
    - seeds: List of seeds
    - n_seeds: Number of seeds if generating

    Returns:
    - DataFrame with results
    """
    print("\n" + "="*70)
    print("TESTING OPTIMIZED MODEL ACROSS MULTIPLE SEEDS")
    print("="*70)

    if seeds is None:
        seeds = generate_seeds('F2_4_optimized_stability', n_seeds)

    print(f"\nTesting optimized model across {len(seeds)} seeds")
    print(f"Optimized parameters:")
    for key, value in best_params.items():
        if key not in ['scale_pos_weight', 'random_state', 'eval_metric']:
            print(f"  {key}: {value}")

    # Prepare data
    y = df[target_col].dropna()
    X = df[features].fillna(df[features].median())
    X = X.loc[y.index]

    results = []

    print("\nRunning tests...")
    for i, seed in enumerate(seeds):
        print(f"Seed {i+1}/{len(seeds)} (seed={seed})...", end=' ')

        # Split with specific seed
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=seed, stratify=y
        )

        # Update params for this seed
        params = best_params.copy()
        scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
        params['scale_pos_weight'] = scale_pos_weight
        params['random_state'] = seed
        params['eval_metric'] = 'logloss'

        # Train model
        model = xgb.XGBClassifier(**params)
        model.fit(X_train, y_train, verbose=False)

        # Evaluate
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]

        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        roc_auc = roc_auc_score(y_test, y_pred_proba)

        from sklearn.metrics import precision_score, recall_score
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)

        print(f"Acc={accuracy:.4f}, F1={f1:.4f}, AUC={roc_auc:.4f}")

        results.append({
            'seed': seed,
            'seed_idx': i + 1,
            'accuracy': accuracy,
            'f1_score': f1,
            'precision': precision,
            'recall': recall,
            'roc_auc': roc_auc
        })

    results_df = pd.DataFrame(results)

    # Compare with baseline
    print("\n" + "="*70)
    print("OPTIMIZED MODEL STABILITY SUMMARY")
    print("="*70)

    for metric in ['accuracy', 'f1_score', 'precision', 'recall', 'roc_auc']:
        mean = results_df[metric].mean()
        std = results_df[metric].std()
        cv = std / mean

        print(f"\n{metric.upper().replace('_', ' ')}:")
        print(f"  Mean: {mean:.4f} ± {std:.4f}")
        print(f"  CV: {cv:.4f} {'✓' if cv < 0.05 else '⚠' if cv < 0.10 else '✗'}")

    return results_df

# ============================================================================
# COMPLETE WORKFLOW
# ============================================================================

def run_complete_stability_optimization(df, target_col, features,
                                       n_variance_seeds=10,
                                       n_optuna_trials=50,
                                       optimization_seed=my_seed,
                                       n_stability_seeds=10):
    """
    Complete workflow: Variance analysis -> Optimization -> Stability test

    Parameters:
    - df: DataFrame
    - target_col: Target variable
    - features: List of features (top 10 recommended)
    - n_variance_seeds: Seeds for initial variance analysis
    - n_optuna_trials: Optuna optimization trials
    - optimization_seed: Seed for optimization
    - n_stability_seeds: Seeds for final stability test

    Returns:
    - Dictionary with all results
    """
    print("="*70)
    print("COMPLETE STABILITY AND OPTIMIZATION PIPELINE")
    print("="*70)
    print(f"Features: {len(features)}")
    print(f"Variance analysis seeds: {n_variance_seeds}")
    print(f"Optuna trials: {n_optuna_trials}")
    print(f"Stability test seeds: {n_stability_seeds}")

    # Step 1: Initial variance analysis (baseline model)
    print("\n" + "="*70)
    print("STEP 1: BASELINE MODEL VARIANCE ANALYSIS")
    print("="*70)

    baseline_seeds = generate_seeds('baseline_variance', n_variance_seeds)
    baseline_results = multiseed_variance_analysis(
        df, target_col, features, baseline_seeds, n_variance_seeds
    )
    visualize_seed_variance(baseline_results)

    # Step 2: Hyperparameter optimization with single seed
    print("\n" + "="*70)
    print("STEP 2: HYPERPARAMETER OPTIMIZATION")
    print("="*70)

    optuna_results = optimize_with_optuna(
        df, target_col, features, n_optuna_trials, optimization_seed
    )
    visualize_optuna_results(optuna_results['study'])

    # Step 3: Test optimized model stability across seeds
    print("\n" + "="*70)
    print("STEP 3: OPTIMIZED MODEL STABILITY TEST")
    print("="*70)

    stability_seeds = generate_seeds('optimized_stability', n_stability_seeds)
    optimized_results = test_optimized_model_stability(
        df, target_col, features, optuna_results['best_params'],
        stability_seeds, n_stability_seeds
    )
    visualize_seed_variance(optimized_results)

    # Final comparison
    print("\n" + "="*70)
    print("FINAL COMPARISON: BASELINE VS OPTIMIZED")
    print("="*70)

    comparison = pd.DataFrame({
        'Metric': ['Accuracy', 'F1-Score', 'ROC-AUC'],
        'Baseline_Mean': [
            baseline_results['accuracy'].mean(),
            baseline_results['f1_score'].mean(),
            baseline_results['roc_auc'].mean()
        ],
        'Baseline_Std': [
            baseline_results['accuracy'].std(),
            baseline_results['f1_score'].std(),
            baseline_results['roc_auc'].std()
        ],
        'Optimized_Mean': [
            optimized_results['accuracy'].mean(),
            optimized_results['f1_score'].mean(),
            optimized_results['roc_auc'].mean()
        ],
        'Optimized_Std': [
            optimized_results['accuracy'].std(),
            optimized_results['f1_score'].std(),
            optimized_results['roc_auc'].std()
        ]
    })

    comparison['Improvement'] = comparison['Optimized_Mean'] - comparison['Baseline_Mean']
    comparison['Stability_Improvement'] = comparison['Baseline_Std'] - comparison['Optimized_Std']

    print(comparison.to_string(index=False))

    # Export results
    baseline_results.to_csv('baseline_variance_results.csv', index=False)
    optimized_results.to_csv('optimized_stability_results.csv', index=False)
    comparison.to_csv('baseline_vs_optimized_comparison.csv', index=False)

    print("\n✓ Results exported to CSV files")

    return {
        'baseline_results': baseline_results,
        'optuna_results': optuna_results,
        'optimized_results': optimized_results,
        'comparison': comparison,
        'best_model': optuna_results['best_model'],
        'best_params': optuna_results['best_params']
    }

# ============================================================================
# USAGE
# ============================================================================

"""
# Get your top 30 features from previous analysis
top_30_features = ['G25_2', 'F1', 'J32', 'J10', 'B4', 'G1', 'G23', 'G20', 'B43', 'J4', 'M21_4', 'A21_2015', 'G25_4', 'C1_2012', 'M10', 'G25_5', 'B42', 'G25_3', 'A10', 'G25_1', 'H30_2', 'J51_3', 'P51', 'J51_1', 'G30_1', 'J3', 'M20', 'A4A_new_w', 'G30_3', 'J51_2']

# Run complete pipeline
results = run_complete_stability_optimization(
    df=analysis_df_clean,  # Your cleaned dataframe
    target_col='F2_4_binary',
    features=top_10_features,
    n_variance_seeds=10,
    n_optuna_trials=50,
    optimization_seed=42,
    n_stability_seeds=10
)

# Access results
best_model = results['best_model']
best_params = results['best_params']
comparison = results['comparison']
"""

results_pipeline = run_complete_pipeline(analysis_df, top_n_features=30)

# Step 3: Extract the cleaned data
df_binary = results_pipeline['cleaned_data']

# Step 4: NOW run the stability optimization with top 30 features
top_30_features =['G25_2', 'F1', 'J32', 'J10', 'B4', 'G1', 'G23', 'G20', 'B43', 'J4', 'M21_4', 'A21_2015', 'G25_4', 'C1_2012', 'M10', 'G25_5', 'B42', 'G25_3', 'A10', 'G25_1', 'H30_2', 'J51_3', 'P51', 'J51_1', 'G30_1', 'J3', 'M20', 'A4A_new_w', 'G30_3', 'J51_2']

results = run_complete_stability_optimization(
    df=df_binary,
    target_col='F2_4_binary',
    features=top_30_features,
    n_variance_seeds=10,
    n_optuna_trials=50,
    optimization_seed=my_seed,
    n_stability_seeds=10
)

# ============================================================================
# CUSTOM SCORING FUNCTION FOR PRECISION-RECALL BALANCE
# ============================================================================

def precision_recall_f2_score(y_true, y_pred):
    """
    Custom metric that prioritizes precision while maintaining recall
    F2 score weights recall higher, but we can create a custom balance
    """
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)

    # Require minimum recall of 65%
    if recall < 0.65:
        return 0.0

    # Weighted F-score: emphasize precision
    # F-beta where beta < 1 emphasizes precision
    beta = 0.5  # Emphasizes precision over recall
    if precision + recall == 0:
        return 0.0

    f_score = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall)
    return f_score

def precision_at_recall_65(y_true, y_pred_proba, min_recall=0.65):
    """
    Find the threshold that gives at least 65% recall, return precision at that point
    """
    precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred_proba)

    # Find thresholds where recall >= min_recall
    valid_idx = np.where(recalls >= min_recall)[0]

    if len(valid_idx) == 0:
        return 0.0

    # Return maximum precision among valid thresholds
    return precisions[valid_idx].max()

# ============================================================================
# PRECISION-FOCUSED OPTUNA OPTIMIZATION
# ============================================================================

def optimize_for_precision(df, target_col, features, n_trials=100, seed=my_seed,
                          min_recall=0.65, target_precision=0.50):
    """
    Optimize XGBoost to maximize precision while maintaining minimum recall

    Parameters:
    - df: DataFrame
    - target_col: Target variable
    - features: List of features
    - n_trials: Number of optimization trials
    - seed: Random seed
    - min_recall: Minimum acceptable recall (default 65%)
    - target_precision: Target precision to achieve (default 50%)

    Returns:
    - Best parameters and results
    """
    print("="*70)
    print("PRECISION-FOCUSED HYPERPARAMETER OPTIMIZATION")
    print("="*70)
    print(f"Goal: Precision > {target_precision*100}% with Recall > {min_recall*100}%")
    print(f"Running {n_trials} trials...\n")

    # Prepare data
    y = df[target_col].dropna()
    X = df[features].fillna(df[features].median())
    X = X.loc[y.index]

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=seed, stratify=y
    )

    base_scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

    # Store results
    trial_results = []

    def objective(trial):
        # Adjusted hyperparameter ranges for better precision
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 400),
            'max_depth': trial.suggest_int('max_depth', 2, 5),  # Shallower trees
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),
            'min_child_weight': trial.suggest_int('min_child_weight', 5, 15),  # Higher = more conservative
            'subsample': trial.suggest_float('subsample', 0.6, 0.9),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),
            'gamma': trial.suggest_float('gamma', 1.0, 10.0),  # Higher = more pruning
            'reg_alpha': trial.suggest_float('reg_alpha', 1.0, 10.0),  # Stronger regularization
            'reg_lambda': trial.suggest_float('reg_lambda', 2.0, 10.0),  # Stronger regularization

            # CRITICAL: Adjust scale_pos_weight to reduce false positives
            'scale_pos_weight_multiplier': trial.suggest_float('scale_pos_weight_multiplier', 0.5, 1.5),

            'random_state': seed,
            'eval_metric': 'logloss'
        }

        # Apply scale_pos_weight multiplier
        actual_scale_pos_weight = base_scale_pos_weight * params['scale_pos_weight_multiplier']
        params['scale_pos_weight'] = actual_scale_pos_weight
        params_for_model = {k: v for k, v in params.items() if k != 'scale_pos_weight_multiplier'}

        # Train model
        model = xgb.XGBClassifier(**params_for_model)
        model.fit(X_train, y_train, verbose=False)

        # Predict on validation set
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]

        # Calculate metrics
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        roc_auc = roc_auc_score(y_test, y_pred_proba)

        # Store trial results
        trial_results.append({
            'trial': trial.number,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'roc_auc': roc_auc,
            'scale_pos_weight': actual_scale_pos_weight
        })

        # Penalty if recall is too low
        if recall < min_recall:
            penalty = (min_recall - recall) * 10  # Heavy penalty
            return precision - penalty

        # Return precision as the objective (what we want to maximize)
        return precision

    # Run optimization
    study = optuna.create_study(
        direction='maximize',
        sampler=optuna.samplers.TPESampler(seed=seed)
    )

    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)

    # Results
    print("\n" + "="*70)
    print("OPTIMIZATION RESULTS")
    print("="*70)

    # Train final model with best parameters
    best_params = study.best_params.copy()
    scale_multiplier = best_params.pop('scale_pos_weight_multiplier')
    best_params['scale_pos_weight'] = base_scale_pos_weight * scale_multiplier
    best_params['random_state'] = seed
    best_params['eval_metric'] = 'logloss'

    best_model = xgb.XGBClassifier(**best_params)
    best_model.fit(X_train, y_train, verbose=False)

    # Evaluate
    y_pred = best_model.predict(X_test)
    y_pred_proba = best_model.predict_proba(X_test)[:, 1]

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba)

    print(f"\nBest Model Performance:")
    print(f"  Precision: {precision:.4f} {'✓' if precision >= target_precision else '✗'} (Target: {target_precision:.2f})")
    print(f"  Recall: {recall:.4f} {'✓' if recall >= min_recall else '✗'} (Min: {min_recall:.2f})")
    print(f"  F1-Score: {f1:.4f}")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  ROC-AUC: {roc_auc:.4f}")

    print(f"\nBest Hyperparameters:")
    for key, value in best_params.items():
        if key not in ['random_state', 'eval_metric']:
            print(f"  {key}: {value}")

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred,
                                target_names=['No Late Fees', 'Late Fees']))

    return {
        'study': study,
        'best_params': best_params,
        'best_model': best_model,
        'trial_results': pd.DataFrame(trial_results),
        'test_metrics': {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'roc_auc': roc_auc
        },
        'X_test': X_test,
        'y_test': y_test,
        'y_pred': y_pred,
        'y_pred_proba': y_pred_proba
    }

# ============================================================================
# THRESHOLD TUNING FOR PRECISION
# ============================================================================

def tune_decision_threshold(model, X_test, y_test, min_recall=0.65):
    """
    Find optimal decision threshold to maximize precision while maintaining recall

    Parameters:
    - model: Trained model
    - X_test: Test features
    - y_test: Test labels
    - min_recall: Minimum acceptable recall

    Returns:
    - Optimal threshold and metrics
    """
    print("\n" + "="*70)
    print("DECISION THRESHOLD TUNING")
    print("="*70)

    y_pred_proba = model.predict_proba(X_test)[:, 1]

    # Calculate precision-recall curve
    precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba)

    # Note: precisions and recalls have one more element than thresholds
    # We need to align them properly

    # Find thresholds where recall >= min_recall
    # Use recalls[:-1] to match thresholds length
    valid_indices = np.where(recalls[:-1] >= min_recall)[0]

    if len(valid_indices) == 0:
        print(f"Warning: No threshold achieves recall >= {min_recall}")
        best_threshold = 0.5
        valid_thresholds = []
        valid_precisions = []
        valid_recalls = []
    else:
        valid_thresholds = thresholds[valid_indices]
        valid_precisions = precisions[valid_indices]
        valid_recalls = recalls[valid_indices]

    if len(valid_thresholds) == 0:
        print(f"Warning: No threshold achieves recall >= {min_recall}")
        best_threshold = 0.5
        best_precision = precisions[0]
        best_recall = recalls[0]
    else:
        # Find threshold with maximum precision
        best_idx = np.argmax(valid_precisions)
        best_threshold = valid_thresholds[best_idx]
        best_precision = valid_precisions[best_idx]
        best_recall = valid_recalls[best_idx]

        print(f"Optimal Threshold: {best_threshold:.4f}")
        print(f"  Precision: {best_precision:.4f}")
        print(f"  Recall: {best_recall:.4f}")
        print(f"  F1-Score: {2 * best_precision * best_recall / (best_precision + best_recall):.4f}")

    # Apply optimal threshold
    y_pred_tuned = (y_pred_proba >= best_threshold).astype(int)

    # Calculate all metrics with tuned threshold
    accuracy_tuned = accuracy_score(y_test, y_pred_tuned)
    precision_tuned = precision_score(y_test, y_pred_tuned)
    recall_tuned = recall_score(y_test, y_pred_tuned)
    f1_tuned = f1_score(y_test, y_pred_tuned)

    print(f"\nTuned Model Performance:")
    print(f"  Accuracy: {accuracy_tuned:.4f}")
    print(f"  Precision: {precision_tuned:.4f}")
    print(f"  Recall: {recall_tuned:.4f}")
    print(f"  F1-Score: {f1_tuned:.4f}")

    print("\nClassification Report (with tuned threshold):")
    print(classification_report(y_test, y_pred_tuned,
                                target_names=['No Late Fees', 'Late Fees']))

    # Visualize precision-recall tradeoff
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # Precision-Recall curve
    ax1 = axes[0]
    ax1.plot(recalls, precisions, 'b-', linewidth=2, label='PR Curve')
    ax1.axhline(y=min_recall, color='r', linestyle='--', label=f'Min Recall = {min_recall}')
    ax1.axvline(x=best_recall, color='g', linestyle='--', alpha=0.5)
    ax1.axhline(y=best_precision, color='g', linestyle='--', alpha=0.5)
    ax1.scatter([best_recall], [best_precision], s=200, c='green',
               marker='*', label=f'Optimal (P={best_precision:.2f}, R={best_recall:.2f})')
    ax1.set_xlabel('Recall')
    ax1.set_ylabel('Precision')
    ax1.set_title('Precision-Recall Curve')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Threshold vs Metrics
    ax2 = axes[1]
    # Recompute for all thresholds
    threshold_range = np.linspace(0, 1, 100)
    precisions_full = []
    recalls_full = []
    f1s_full = []

    for thresh in threshold_range:
        y_pred_temp = (y_pred_proba >= thresh).astype(int)
        if y_pred_temp.sum() > 0:  # At least one positive prediction
            precisions_full.append(precision_score(y_test, y_pred_temp, zero_division=0))
            recalls_full.append(recall_score(y_test, y_pred_temp, zero_division=0))
            f1s_full.append(f1_score(y_test, y_pred_temp, zero_division=0))
        else:
            precisions_full.append(0)
            recalls_full.append(0)
            f1s_full.append(0)

    ax2.plot(threshold_range, precisions_full, 'b-', label='Precision', linewidth=2)
    ax2.plot(threshold_range, recalls_full, 'r-', label='Recall', linewidth=2)
    ax2.plot(threshold_range, f1s_full, 'g-', label='F1-Score', linewidth=2)
    ax2.axvline(x=best_threshold, color='purple', linestyle='--',
               linewidth=2, label=f'Optimal Threshold = {best_threshold:.3f}')
    ax2.axvline(x=0.5, color='gray', linestyle=':', alpha=0.5, label='Default = 0.5')
    ax2.set_xlabel('Decision Threshold')
    ax2.set_ylabel('Score')
    ax2.set_title('Metrics vs Decision Threshold')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    return {
        'optimal_threshold': best_threshold,
        'precision': precision_tuned,
        'recall': recall_tuned,
        'f1': f1_tuned,
        'accuracy': accuracy_tuned,
        'y_pred_tuned': y_pred_tuned
    }

# ============================================================================
# VISUALIZE OPTIMIZATION PROGRESS
# ============================================================================

def visualize_precision_optimization(trial_results_df):
    """Visualize how precision and recall evolved during optimization"""

    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # 1. Precision over trials
    ax1 = axes[0, 0]
    ax1.scatter(trial_results_df['trial'], trial_results_df['precision'],
               alpha=0.6, s=50)
    ax1.plot(trial_results_df['trial'], trial_results_df['precision'].cummax(),
            'r-', linewidth=2, label='Best So Far')
    ax1.axhline(y=0.5, color='green', linestyle='--', label='Target (50%)')
    ax1.set_xlabel('Trial')
    ax1.set_ylabel('Precision')
    ax1.set_title('Precision Over Optimization Trials')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 2. Precision vs Recall scatter
    ax2 = axes[0, 1]
    scatter = ax2.scatter(trial_results_df['recall'], trial_results_df['precision'],
                         c=trial_results_df['f1'], cmap='viridis', s=50, alpha=0.6)
    ax2.axhline(y=0.5, color='green', linestyle='--', label='Target Precision')
    ax2.axvline(x=0.65, color='orange', linestyle='--', label='Min Recall')
    ax2.set_xlabel('Recall')
    ax2.set_ylabel('Precision')
    ax2.set_title('Precision-Recall Trade-off')
    plt.colorbar(scatter, ax=ax2, label='F1-Score')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # 3. Scale pos weight effect
    ax3 = axes[1, 0]
    ax3.scatter(trial_results_df['scale_pos_weight'],
               trial_results_df['precision'], alpha=0.6, s=50)
    ax3.set_xlabel('Scale Pos Weight')
    ax3.set_ylabel('Precision')
    ax3.set_title('Effect of Scale Pos Weight on Precision')
    ax3.grid(True, alpha=0.3)

    # 4. Best trials table
    ax4 = axes[1, 1]
    ax4.axis('off')

    # Get top 5 trials by precision with recall >= 0.65
    valid_trials = trial_results_df[trial_results_df['recall'] >= 0.65]
    top_5 = valid_trials.nlargest(5, 'precision')[['trial', 'precision', 'recall', 'f1', 'roc_auc']]

    table_data = []
    for _, row in top_5.iterrows():
        table_data.append([
            f"{int(row['trial'])}",
            f"{row['precision']:.3f}",
            f"{row['recall']:.3f}",
            f"{row['f1']:.3f}",
            f"{row['roc_auc']:.3f}"
        ])

    table = ax4.table(cellText=table_data,
                     colLabels=['Trial', 'Precision', 'Recall', 'F1', 'AUC'],
                     cellLoc='center',
                     loc='center',
                     bbox=[0, 0, 1, 1])
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2)
    ax4.set_title('Top 5 Trials (Recall ≥ 65%)', pad=20)

    plt.tight_layout()
    plt.show()

# ============================================================================
# COMPLETE WORKFLOW
# ============================================================================

def run_precision_optimization_pipeline(df, target_col, features,
                                       n_trials=100, seed=my_seed):
    """
    Complete pipeline for precision-focused optimization

    Parameters:
    - df: DataFrame
    - target_col: Target variable
    - features: List of features
    - n_trials: Number of Optuna trials
    - seed: Random seed
    """
    print("="*70)
    print("COMPLETE PRECISION OPTIMIZATION PIPELINE")
    print("="*70)

    # Step 1: Optimize hyperparameters for precision
    results = optimize_for_precision(
        df, target_col, features,
        n_trials=n_trials,
        seed=seed,
        min_recall=0.65,
        target_precision=0.50
    )

    # Step 2: Visualize optimization process
    print("\nVisualizing optimization progress...")
    visualize_precision_optimization(results['trial_results'])

    # Step 3: Tune decision threshold
    threshold_results = tune_decision_threshold(
        results['best_model'],
        results['X_test'],
        results['y_test'],
        min_recall=0.65
    )

    # Step 4: Compare results
    print("\n" + "="*70)
    print("FINAL COMPARISON")
    print("="*70)

    comparison = pd.DataFrame({
        'Method': ['Default Threshold (0.5)', 'Tuned Threshold'],
        'Precision': [results['test_metrics']['precision'], threshold_results['precision']],
        'Recall': [results['test_metrics']['recall'], threshold_results['recall']],
        'F1-Score': [results['test_metrics']['f1'], threshold_results['f1']],
        'Accuracy': [results['test_metrics']['accuracy'], threshold_results['accuracy']]
    })

    print(comparison.to_string(index=False))

    # Export results
    results['trial_results'].to_csv('precision_optimization_trials.csv', index=False)
    comparison.to_csv('precision_threshold_comparison.csv', index=False)

    print("\n✓ Results exported to CSV files")

    return {
        'optimization_results': results,
        'threshold_results': threshold_results,
        'comparison': comparison
    }

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import (accuracy_score, f1_score, roc_auc_score,
                             precision_score, recall_score, classification_report,
                             precision_recall_curve, make_scorer)
# Generate your deterministic seed
def md5_hash(input_string):
    """Generates an MD5 hash from a given string."""
    md5_hasher = hashlib.md5()
    md5_hasher.update(input_string.encode('utf-8'))
    return md5_hasher.hexdigest()

# Generate seed from your name
input_string = "Qihang"
hashed_value = md5_hash(input_string)
number = int(hashed_value, 16)

# Keep within valid range for random seed
my_seed = number % (2**31 - 1)
print(f"Using seed: {my_seed}")

# Your top 10 features
#top_20_features = ['G25_2', 'F1', 'J32', 'J10', 'B4', 'G1', 'G23', 'G20', 'B43', 'J4', 'M21_4', 'A21_2015', 'G25_4', 'C1_2012', 'M10', 'G25_5', 'B42', 'G25_3', 'A10', 'G25_1']

# Run precision-focused optimization with YOUR seed
precision_results = run_precision_optimization_pipeline(
    df=df_binary,
    target_col='F2_4_binary',
    features=top_30_features,
    n_trials=100,
    seed=my_seed  # Using your hash-based seed
)


# Access best model
best_model = precision_results['optimization_results']['best_model']
optimal_threshold = precision_results['threshold_results']['optimal_threshold']

print(f"\n{'='*70}")
print("MODEL READY FOR DEPLOYMENT")
print('='*70)
print(f"Optimal decision threshold: {optimal_threshold:.4f}")
print(f"Final Precision: {precision_results['threshold_results']['precision']:.4f}")
print(f"Final Recall: {precision_results['threshold_results']['recall']:.4f}")

"""
Compare Optimized XGBoost vs Random Forest vs Logistic Regression
For Credit Card Default Prediction
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, f1_score, roc_auc_score,
                             precision_score, recall_score, classification_report,
                             confusion_matrix, roc_curve)
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
import warnings
from sklearn.model_selection import RandomizedSearchCV

warnings.filterwarnings('ignore')

def compare_all_models(df, target_col, features, xgboost_model, xgboost_threshold, seed=my_seed):
    """
    Compare optimized XGBoost with Random Forest and Logistic Regression

    Parameters:
    - df: DataFrame
    - target_col: Target variable
    - features: List of features
    - xgboost_model: Already optimized XGBoost model
    - xgboost_threshold: Optimal threshold from tuning
    - seed: Random seed

    Returns:
    - Dictionary with all results
    """
    print("="*70)
    print("COMPREHENSIVE MODEL COMPARISON")
    print("="*70)
    print("Models: XGBoost (Optimized) vs Random Forest vs Logistic Regression")

    # Prepare data
    y = df[target_col].dropna()
    X = df[features].fillna(df[features].median())
    X = X.loc[y.index]

    # Split data (same split for fair comparison)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=seed, stratify=y
    )

    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

    results = {}

    # ========================================================================
    # MODEL 1: XGBoost (Already Optimized)
    # ========================================================================
    print("\n" + "="*70)
    print("MODEL 1: XGBOOST CLASSIFIER (OPTIMIZED)")
    print("="*70)

    y_pred_proba_xgb = xgboost_model.predict_proba(X_test)[:, 1]
    y_pred_xgb = (y_pred_proba_xgb >= xgboost_threshold).astype(int)

    results['xgboost'] = evaluate_model(
        'XGBoost', y_test, y_pred_xgb, y_pred_proba_xgb,
        xgboost_model, X_train, y_train
    )

    # ========================================================================
    # MODEL 2: Random Forest (with class_weight)
    # ========================================================================
    print("\n" + "="*70)
    print("MODEL 2: RANDOM FOREST CLASSIFIER")
    print("="*70)
    print("Performing grid search for optimal hyperparameters...")

    rf_param_grid = {
        'n_estimators': [200, 300, 400],
        'max_depth': [3, 5, 7, 10],
        'min_samples_split': [10, 20, 30],
        'min_samples_leaf': [5, 10, 15],
        'max_features': ['sqrt', 'log2'],
        'class_weight': ['balanced', {0: 1, 1: 2}, {0: 1, 1: 2.5}]
    }

    rf_base = RandomForestClassifier(random_state=seed)

    rf_grid = RandomizedSearchCV(
        rf_base,
        rf_param_grid,
        n_iter=30,
        cv=5,
        scoring='roc_auc',
        n_jobs=-1,
        verbose=1
    )

    rf_grid.fit(X_train, y_train)

    print(f"\nBest Random Forest Parameters:")
    for key, value in rf_grid.best_params_.items():
        print(f"  {key}: {value}")

    rf_best = rf_grid.best_estimator_
    y_pred_rf = rf_best.predict(X_test)
    y_pred_proba_rf = rf_best.predict_proba(X_test)[:, 1]

    results['random_forest'] = evaluate_model(
        'Random Forest', y_test, y_pred_rf, y_pred_proba_rf,
        rf_best, X_train, y_train
    )

    # Tune threshold for Random Forest
    rf_threshold = find_best_threshold(y_test, y_pred_proba_rf, min_recall=0.65)
    y_pred_rf_tuned = (y_pred_proba_rf >= rf_threshold).astype(int)

    results['random_forest_tuned'] = evaluate_model(
        'Random Forest (Tuned)', y_test, y_pred_rf_tuned, y_pred_proba_rf,
        rf_best, X_train, y_train
    )
    results['random_forest_tuned']['threshold'] = rf_threshold

    # ========================================================================
    # MODEL 3: Logistic Regression (with regularization)
    # ========================================================================
    print("\n" + "="*70)
    print("MODEL 3: LOGISTIC REGRESSION")
    print("="*70)
    print("Performing grid search for optimal hyperparameters...")

    # Scale features for logistic regression
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    lr_param_grid = {
        'C': [0.001, 0.01, 0.1, 0.5, 1.0, 5.0, 10.0],
        'penalty': ['l1', 'l2'],
        'solver': ['liblinear'],
        'class_weight': ['balanced', {0: 1, 1: 2}, {0: 1, 1: 3}]
    }

    lr_base = LogisticRegression(random_state=seed, max_iter=1000)

    lr_grid = RandomizedSearchCV(
        lr_base,
        lr_param_grid,
        n_iter=10,
        cv=5,
        scoring='roc_auc',
        n_jobs=-1,
        verbose=1
    )

    lr_grid.fit(X_train_scaled, y_train)

    print(f"\nBest Logistic Regression Parameters:")
    for key, value in lr_grid.best_params_.items():
        print(f"  {key}: {value}")

    lr_best = lr_grid.best_estimator_
    y_pred_lr = lr_best.predict(X_test_scaled)
    y_pred_proba_lr = lr_best.predict_proba(X_test_scaled)[:, 1]

    results['logistic'] = evaluate_model(
        'Logistic Regression', y_test, y_pred_lr, y_pred_proba_lr,
        lr_best, X_train_scaled, y_train
    )

    # Tune threshold for Logistic Regression
    lr_threshold = find_best_threshold(y_test, y_pred_proba_lr, min_recall=0.65)
    y_pred_lr_tuned = (y_pred_proba_lr >= lr_threshold).astype(int)

    results['logistic_tuned'] = evaluate_model(
        'Logistic Regression (Tuned)', y_test, y_pred_lr_tuned, y_pred_proba_lr,
        lr_best, X_train_scaled, y_train
    )
    results['logistic_tuned']['threshold'] = lr_threshold

    # ========================================================================
    # COMPREHENSIVE COMPARISON
    # ========================================================================
    print("\n" + "="*70)
    print("COMPREHENSIVE COMPARISON TABLE")
    print("="*70)

    comparison_data = []
    for model_name, model_results in results.items():
        comparison_data.append({
            'Model': model_name.replace('_', ' ').title(),
            'Precision': f"{model_results['precision']:.4f}",
            'Recall': f"{model_results['recall']:.4f}",
            'F1-Score': f"{model_results['f1']:.4f}",
            'Accuracy': f"{model_results['accuracy']:.4f}",
            'ROC-AUC': f"{model_results['roc_auc']:.4f}",
            'CV ROC-AUC': f"{model_results['cv_roc_auc']:.4f} ± {model_results['cv_roc_auc_std']:.4f}"
        })

    comparison_df = pd.DataFrame(comparison_data)
    print(comparison_df.to_string(index=False))

    # Identify best model
    print("\n" + "="*70)
    print("WINNER DETERMINATION")
    print("="*70)

    # Extract numeric values for comparison
    numeric_comparison = pd.DataFrame([
        {
            'Model': name,
            'Precision': res['precision'],
            'Recall': res['recall'],
            'F1': res['f1'],
            'ROC-AUC': res['roc_auc']
        }
        for name, res in results.items()
    ])

    best_precision = numeric_comparison.loc[numeric_comparison['Precision'].idxmax()]
    best_f1 = numeric_comparison.loc[numeric_comparison['F1'].idxmax()]
    best_roc_auc = numeric_comparison.loc[numeric_comparison['ROC-AUC'].idxmax()]

    print(f"Best Precision: {best_precision['Model']} ({best_precision['Precision']:.4f})")
    print(f"Best F1-Score: {best_f1['Model']} ({best_f1['F1']:.4f})")
    print(f"Best ROC-AUC: {best_roc_auc['Model']} ({best_roc_auc['ROC-AUC']:.4f})")

    # Overall recommendation
    print("\n" + "="*70)
    print("FINAL RECOMMENDATION")
    print("="*70)

    # Score each model (higher is better)
    for name, res in results.items():
        score = (
            res['precision'] * 1 +  # Weight precision heavily
            res['recall'] * 1 +   # Weight recall moderately
            res['roc_auc'] * 1      # Weight AUC
        ) / 3
        results[name]['overall_score'] = score

    best_overall = max(results.items(), key=lambda x: x[1]['overall_score'])

    print(f"✓ RECOMMENDED MODEL: {best_overall[0].replace('_', ' ').title()}")
    print(f"  Overall Score: {best_overall[1]['overall_score']:.4f}")
    print(f"  Precision: {best_overall[1]['precision']:.4f}")
    print(f"  Recall: {best_overall[1]['recall']:.4f}")
    print(f"  ROC-AUC: {best_overall[1]['roc_auc']:.4f}")

    # Visualizations
    visualize_model_comparison(results, y_test,
                               y_pred_proba_xgb, y_pred_proba_rf, y_pred_proba_lr)

    # Export
    comparison_df.to_csv('final_model_comparison.csv', index=False)
    print("\n✓ Results exported to: final_model_comparison.csv")

    return results, comparison_df

def evaluate_model(model_name, y_test, y_pred, y_pred_proba, model, X_train, y_train):
    """Evaluate a single model and return metrics"""

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    roc_auc = roc_auc_score(y_test, y_pred_proba)

    # Cross-validation
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')

    print(f"\n{model_name} Performance:")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall: {recall:.4f}")
    print(f"  F1-Score: {f1:.4f}")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  ROC-AUC: {roc_auc:.4f}")
    print(f"  CV ROC-AUC: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}")

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred,
                                target_names=['No Late Fees', 'Late Fees']))

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'roc_auc': roc_auc,
        'cv_roc_auc': cv_scores.mean(),
        'cv_roc_auc_std': cv_scores.std(),
        'y_pred': y_pred,
        'y_pred_proba': y_pred_proba
    }

def find_best_threshold(y_test, y_pred_proba, min_recall=0.65):
    """Find optimal threshold for precision given minimum recall"""
    from sklearn.metrics import precision_recall_curve

    precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba)

    valid_indices = np.where(recalls[:-1] >= min_recall)[0]

    if len(valid_indices) == 0:
        return 0.5

    valid_thresholds = thresholds[valid_indices]
    valid_precisions = precisions[valid_indices]

    best_idx = np.argmax(valid_precisions)
    return valid_thresholds[best_idx]

def visualize_model_comparison(results, y_test, y_pred_proba_xgb,
                               y_pred_proba_rf, y_pred_proba_lr):
    """Create comprehensive comparison visualizations"""

    fig = plt.figure(figsize=(18, 12))
    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

    # 1. ROC Curves Comparison
    ax1 = fig.add_subplot(gs[0, :2])

    fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_proba_xgb)
    fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)
    fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_proba_lr)

    ax1.plot(fpr_xgb, tpr_xgb, 'b-', linewidth=2,
            label=f'XGBoost (AUC={results["xgboost"]["roc_auc"]:.3f})')
    ax1.plot(fpr_rf, tpr_rf, 'g-', linewidth=2,
            label=f'Random Forest (AUC={results["random_forest_tuned"]["roc_auc"]:.3f})')
    ax1.plot(fpr_lr, tpr_lr, 'r-', linewidth=2,
            label=f'Logistic Reg (AUC={results["logistic_tuned"]["roc_auc"]:.3f})')
    ax1.plot([0, 1], [0, 1], 'k--', alpha=0.3)
    ax1.set_xlabel('False Positive Rate')
    ax1.set_ylabel('True Positive Rate')
    ax1.set_title('ROC Curves Comparison', fontsize=14, fontweight='bold')
    ax1.legend(loc='lower right')
    ax1.grid(True, alpha=0.3)

    # 2. Metrics Comparison Bar Chart
    ax2 = fig.add_subplot(gs[0, 2])

    metrics = ['Precision', 'Recall', 'F1-Score', 'ROC-AUC']
    xgb_vals = [results['xgboost']['precision'], results['xgboost']['recall'],
                results['xgboost']['f1'], results['xgboost']['roc_auc']]
    rf_vals = [results['random_forest_tuned']['precision'],
               results['random_forest_tuned']['recall'],
               results['random_forest_tuned']['f1'],
               results['random_forest_tuned']['roc_auc']]
    lr_vals = [results['logistic_tuned']['precision'],
               results['logistic_tuned']['recall'],
               results['logistic_tuned']['f1'],
               results['logistic_tuned']['roc_auc']]

    x = np.arange(len(metrics))
    width = 0.25

    ax2.bar(x - width, xgb_vals, width, label='XGBoost', color='blue', alpha=0.7)
    ax2.bar(x, rf_vals, width, label='Random Forest', color='green', alpha=0.7)
    ax2.bar(x + width, lr_vals, width, label='Logistic Reg', color='red', alpha=0.7)

    ax2.set_xticks(x)
    ax2.set_xticklabels(metrics, rotation=45, ha='right')
    ax2.set_ylabel('Score')
    ax2.set_title('Metrics Comparison', fontweight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3, axis='y')
    ax2.set_ylim([0, 1])

    # 3-5. Confusion Matrices
    for idx, (name, key) in enumerate([('XGBoost', 'xgboost'),
                                        ('Random Forest', 'random_forest_tuned'),
                                        ('Logistic Reg', 'logistic_tuned')]):
        ax = fig.add_subplot(gs[1, idx])
        cm = confusion_matrix(y_test, results[key]['y_pred'])
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False)
        ax.set_title(f'{name}\nConfusion Matrix', fontweight='bold')
        ax.set_xlabel('Predicted')
        ax.set_ylabel('Actual')

    # 6. Precision-Recall Comparison
    ax6 = fig.add_subplot(gs[2, :])

    from sklearn.metrics import precision_recall_curve

    prec_xgb, rec_xgb, _ = precision_recall_curve(y_test, y_pred_proba_xgb)
    prec_rf, rec_rf, _ = precision_recall_curve(y_test, y_pred_proba_rf)
    prec_lr, rec_lr, _ = precision_recall_curve(y_test, y_pred_proba_lr)

    ax6.plot(rec_xgb, prec_xgb, 'b-', linewidth=2, label='XGBoost')
    ax6.plot(rec_rf, prec_rf, 'g-', linewidth=2, label='Random Forest')
    ax6.plot(rec_lr, prec_lr, 'r-', linewidth=2, label='Logistic Regression')
    ax6.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Target Precision')
    ax6.axvline(x=0.65, color='orange', linestyle='--', alpha=0.5, label='Min Recall')
    ax6.set_xlabel('Recall')
    ax6.set_ylabel('Precision')
    ax6.set_title('Precision-Recall Curves Comparison', fontsize=14, fontweight='bold')
    ax6.legend()
    ax6.grid(True, alpha=0.3)

    plt.suptitle('Comprehensive Model Comparison: Credit Card Default Prediction',
                fontsize=16, fontweight='bold', y=0.995)

    plt.savefig('model_comparison_visualization.png', dpi=300, bbox_inches='tight')
    print("\n✓ Visualization saved to: model_comparison_visualization.png")
    plt.show()

final_results, comparison_table = compare_all_models(
    df=df_binary,
    target_col='F2_4_binary',
    features=top_30_features,
    xgboost_model=best_model,
    xgboost_threshold=optimal_threshold,
    seed=my_seed
)

# Access results
best_model_name = max(final_results.items(),
                     key=lambda x: x[1]['overall_score'])[0]
print(f"\nBest Model: {best_model_name}")

